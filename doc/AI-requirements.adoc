= AI Requirements
:icons: font

*NOTE*: this code examples are from ML-Agents 0.15.1 so they might change until first release.

== Technology

1. https://github.com/Unity-Technologies/ml-agents[ML-Agents] needs to release Version 1.0, because all previous files 
require local projects, which are difficult to check in an existing project. Version from 1.0 will support installation
by the `Unity Package Manager`.

2. https://github.com/TreeKid/BrainAcademy[BrainAcademy] by https://github.com/TreeKid[TreeKid] (Juri) needs to support
ML-Agents 1.0. This is due to massive changes from version to version in ML-Agents, which currently has no backwards 
compability at all. `BrainAcademy` runs on ML-Agents 0.13 for now.

== General Definitions

=== Output

_Output_ from now on is consider output from the game to the `Academy`/`Agent` in the Unity Project.

=== Observation

_Observation_ are the data send from `Academy`/`Agent` in Unity to the `unity_environment` in `BrainAcademy`. There are 
two types of observation.

1. Vector Observation: These are single float values, which should range from `[0:1]` or `[-1,1]` and if not other possible
all finite floats.

2. Visual Observation: These are matrices with represents RGB-pixels. So in the `M x N`-matrix each item is three 
dimensional vector. This will probably not be used in this project.

=== Actions

This is calculated by the neuronal network and will be one decision for a character during fighting in this context.
The type is going to be a integer in `[0:n]`, where `n` is the number of choices.

== Unity ML-Agents Components

To integrate ML-Agents into a unity project it needs two components.

=== Academy

This is mostly relevant for training. It will setup an environment for the training process. It will be controlled from
`BrainAcademy` and should have no purpose during production.

=== Agent

The `Agent` _(maybe differently called in ML-Agents 1.0)_ is component of the NPC. It collects all the observation and
will receive the actions. In production it will also contain a neuronal network. The `Agent` is the API for the _game code_.
For this project we will use agents, that will ask the neuronal network for a decision, when won't have continuous 
decision making.

== Communication Requirements

This section will set the requirements for the communication between the three essentials parts _game code_, 
_unity ml-agents_ und `BrainAcademy`.

=== Game Code to Unity ML Agents

This is probably the most import part as will have the most influence on if the AI is going to work. The _game code_ 
not make any direct request or information sharing other than calling for a decision. This will kept the agents much more
flexible, so it will be possible to changes agents quickly and develop different agents for different solutions.

So therefor the _game code_ should provide information sharing methods. The `CollectObservation()` should be able to ask
for a lot of different information. E.g "Give me all enemies." The cropping of the data should happen in the `BrainAcademy`.
It should be possible to ask for everything. This way training speed would increase drastically as when won't need new 
Unity builds, but could change it easily in the `BrainAcademy`. This means for an enemy it should provide a public accessor
which wraps the enemy into a data object.

=== Unity ML Agents to BrainAcademy

One of the crucial parts of developing an neuronal network with reinforcement learning is the input it gets. In order
to keep the size small and enable fast decision making we decided not use any visual observation. In addition to that
the *input for ONE network must always have the same dimension* meaning the neuronal network needs have same choices every
time no matter if this choice is valid or possible. So the _game code_ needs to validate the decision. So the neuronal
network can learn during training, that this decision has no impact and will learn not use it.

There is another approach which catches invalid decisions before hand, but this would only increase the data send from
and forth between all the parts, so this might not be an option, but could be reconsider during training.
****
Agent::

A decision for an agent is request with `Agent.RequestDecision()` as you can see it has no parameters. So in order to get
the observation we need to implement a `CollectObservations(VectorSensor sensor)`. So has the Agent is a component of the
NPC it will have access to other component.

In addition to the decisions needs to have same dimension every time, the observation must have the same dimension as well.
This could be tricky for example in mass battles where multiple allies and enemies are around. But we should try to collect
as much as possible and do any of the data truncating in the `BrainAcademy`.

To further reduce the size of the network and there for speed up the training process, the agent will have no memory.

The agents needs to send observation to the network even when a decision is not required in order to increase accuracies 
and speed of the decision making.
****

==== Observation

One of the most import information the agent can get is about himself. But this should be easy as the agent is a 
component of the NPC. We only need to make sure the agent has access to fields like:

[NOTE]
====
.Fields required
- Health
- Position _Maybe not required if we work with relative positions, which would be more general and easier to learn._


.Fields maybe required
- Inventory _This would increase observation dimensional enormously, but could be required for decisions like taking a 
healt potion._
- Level
====

Another huge impact of the training will be the information about it's environment. It needs to know, where it's allies
and/or enemies are and how their status is.

[NOTE]
====
.Environment Information
* List of `n` nearest enemies
    ** Flag for visible or not
    ** (relative) position (last know relative position if not visible)
    ** movement vector
    ** Health
    ** Class (e.g. mage, warrior, thief)
    ** visible equipment
* List of `m` nearest allies
    ** Flag for visible or not
    ** (relative) position (last know relative position if not visible)
    ** movement vector
    ** Health
    ** Class (e.g. mage, warrior, thief)
    ** visible equipment
====

=== BrainAcademy to Unity ML-Agents and than _game code_

The workflow back is pretty straight forward. The neuronal network will make a decision and pass this on. Probably as an
integer value which will be an index in a list of decisions. This will be passed on the NPC which should have something
like a decision handler and this will trigger internal routines.

==== Decisions

In general the NPC should be allowed to make the same decisions as a human player. So each agents needs a fixed list of 
decision he is allowed to do. These could be:

[NOTE]
====
.List of possible decisions
* Run away -> calling a routine to run away from the fight
* Run towards <Character> -> calls a routine to run as close as possible to the character even though this might move 
during routine
* sword attack 1
* sword attack 2
* [...]
* magic attack 1
* [...]
* potion 1
* [...]
====

=== Rewards

The rewards will be set in the `BrainAcademy` for faster developing circle. As they will be very different for each agent
it would be not sensible to try to describe them here.

== Production Neuronal Network

The neuronal network must be exported as `.nn` for production. This step is mission critical.